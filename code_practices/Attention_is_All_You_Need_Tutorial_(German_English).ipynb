{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention is All You Need Tutorial (German-English)",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwee1017/assignment/blob/main/code_practices/Attention_is_All_You_Need_Tutorial_(German_English).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 1Ô∏è‚É£ Google Drive ÎßàÏö¥Ìä∏\n",
        "# ====================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/llama3_korean_lora\""
      ],
      "metadata": {
        "id": "GeJlLjuvd0-K",
        "outputId": "0f7c4199-41a3-4f72-ddbb-a39c06db5096",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 2Ô∏è‚É£ ÌïÑÏàò Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò\n",
        "# ====================================================\n",
        "!pip install -q datasets transformers peft accelerate bitsandbytes torch pandas tqdm"
      ],
      "metadata": {
        "id": "BREc40rXd7uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 3Ô∏è‚É£ Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨\n",
        "# ====================================================\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "import pandas as pd\n",
        "\n",
        "print(\"üìö Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ï§ë...\")\n",
        "\n",
        "# ÌïúÍµ≠Ïñ¥ QA Î∞è ÎåÄÌôî Îç∞Ïù¥ÌÑ∞ÏÖã 3Ï¢Ö\n",
        "koalpaca = load_dataset(\"beomi/KoAlpaca-v1.1\", split=\"train\")\n",
        "kobest = load_dataset(\"jinmang2/korean_chatbot_qa\", split=\"train\")\n",
        "koconv = load_dataset(\"Ammad1Ali/Korean-conversational-dataset\", split=\"train\")\n",
        "\n",
        "# ÏÉòÌîåÎßÅ (Ï¥ù 7000Í∞ú)\n",
        "koalpaca = koalpaca.shuffle(seed=42).select(range(min(2500, len(koalpaca))))\n",
        "kobest = kobest.shuffle(seed=42).select(range(min(2500, len(kobest))))\n",
        "koconv = koconv.shuffle(seed=42).select(range(min(2000, len(koconv))))\n",
        "\n",
        "# Ï†ÑÏ≤òÎ¶¨ Ìï®Ïàò\n",
        "def format_data(example):\n",
        "    user = (\n",
        "        example.get(\"instruction\")\n",
        "        or example.get(\"input\")\n",
        "        or example.get(\"question\")\n",
        "        or example.get(\"context\")\n",
        "        or \"\"\n",
        "    )\n",
        "    assistant = (\n",
        "        example.get(\"output\")\n",
        "        or example.get(\"answer\")\n",
        "        or example.get(\"response\")\n",
        "        or \"\"\n",
        "    )\n",
        "    text = f\"### User: {user.strip()}\\n### Assistant: {assistant.strip()}\"\n",
        "    return {\"text\": text}\n",
        "\n",
        "dataset = concatenate_datasets([koalpaca, kobest, koconv]).map(format_data)\n",
        "dataset = dataset.filter(lambda x: len(x[\"text\"].strip()) > 0)\n",
        "df = pd.DataFrame(dataset)\n",
        "df.drop_duplicates(subset=[\"text\"], inplace=True)\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"‚úÖ Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ ÏôÑÎ£å. ÏÉòÌîå Ïàò: {len(df)}\")"
      ],
      "metadata": {
        "id": "RV60vKdkeB86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 4Ô∏è‚É£ Î™®Îç∏ Î∞è LoRA ÏÑ§Ï†ï\n",
        "# ====================================================\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# EncoderDecoderCache Î¨∏Ï†ú ÌöåÌîºÏö© Ìå®Ïπò\n",
        "try:\n",
        "    from transformers import EncoderDecoderCache\n",
        "except ImportError:\n",
        "    EncoderDecoderCache = None\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "MODEL_NAME = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"‚úÖ Î™®Îç∏ & LoRA ÏÑ§Ï†ï ÏôÑÎ£å\")"
      ],
      "metadata": {
        "id": "RZxP3QVveFV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 5Ô∏è‚É£ ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï Î∞è Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ\n",
        "# ====================================================\n",
        "from datasets import Dataset\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "raw_dataset = Dataset.from_pandas(df)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "tokenized_dataset = raw_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "FNUcam60eH5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 6Ô∏è‚É£ ÌïôÏäµ ÏÑ§Ï†ï\n",
        "# ====================================================\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=SAVE_DIR,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=50,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        "    evaluation_strategy=\"no\"\n",
        ")"
      ],
      "metadata": {
        "id": "EjLXK8_deKuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 7Ô∏è‚É£ TrainerÎ°ú ÌïôÏäµ\n",
        "# ====================================================\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "print(\"üöÄ ÌïôÏäµ ÏãúÏûë...\")\n",
        "trainer.train()\n",
        "print(\"‚úÖ ÌïôÏäµ ÏôÑÎ£å!\")"
      ],
      "metadata": {
        "id": "XdJbHjD1eMlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 8Ô∏è‚É£ Î™®Îç∏ Ï†ÄÏû•\n",
        "# ====================================================\n",
        "model.save_pretrained(SAVE_DIR)\n",
        "tokenizer.save_pretrained(SAVE_DIR)\n",
        "print(f\"üíæ LoRA Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å ‚Üí {SAVE_DIR}\")"
      ],
      "metadata": {
        "id": "5JvBZKiZeOuF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}