{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention is All You Need Tutorial (German-English)",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwee1017/assignment/blob/main/code_practices/Attention_is_All_You_Need_Tutorial_(German_English).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
        "SAVE_DIR = \"/content/drive/MyDrive/LoRA-Korean-Chat\"\n",
        "print(f\"âœ… ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {SAVE_DIR}\")"
      ],
      "metadata": {
        "id": "4pVWVlXdcKej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 1ï¸âƒ£ í™˜ê²½ ì„¸íŒ…\n",
        "# ====================================================\n",
        "!pip install -q transformers accelerate peft bitsandbytes datasets\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "print(\"âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "yeEBLolfbqum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 2ï¸âƒ£ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
        "# ====================================================\n",
        "print(\"ğŸ“š ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
        "\n",
        "# KoAlpaca + KoChatGPT ìŠ¤íƒ€ì¼ QA ë°ì´í„°\n",
        "koalpaca = load_dataset(\"beomi/KoAlpaca-v1.1\", split=\"train\")\n",
        "kobest = load_dataset(\"jinmang2/korean_chatbot_qa\", split=\"train\")\n",
        "\n",
        "# 7000ê°œë§Œ ìƒ˜í”Œë§\n",
        "koalpaca = koalpaca.shuffle(seed=42).select(range(min(3500, len(koalpaca))))\n",
        "kobest = kobest.shuffle(seed=42).select(range(min(3500, len(kobest))))\n",
        "\n",
        "def format_data(example):\n",
        "    user = example.get(\"instruction\") or example.get(\"input\") or example.get(\"question\", \"\")\n",
        "    assistant = example.get(\"output\") or example.get(\"answer\", \"\")\n",
        "    text = f\"### User: {user.strip()}\\n### Assistant: {assistant.strip()}\"\n",
        "    return {\"text\": text}\n",
        "\n",
        "dataset = concatenate_datasets([koalpaca, kobest]).map(format_data)\n",
        "dataset = dataset.filter(lambda x: len(x[\"text\"].strip()) > 0)\n",
        "\n",
        "print(f\"âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ. ìƒ˜í”Œ ìˆ˜: {len(dataset)}\")"
      ],
      "metadata": {
        "id": "EVZADUZIbvmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 3ï¸âƒ£ í† í¬ë‚˜ì´ì € & ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "# ====================================================\n",
        "model_name = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# pad token ì„¤ì • (ê²½ê³  ë°©ì§€ìš©)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "print(\"âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "4SslPut3bzs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 4ï¸âƒ£ LoRA êµ¬ì„±\n",
        "# ====================================================\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"âœ… LoRA êµ¬ì„± ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "m7YYRhgLb2Lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 5ï¸âƒ£ í† í¬ë‚˜ì´ì§•\n",
        "# ====================================================\n",
        "def tokenize(example):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "dataset = dataset.map(tokenize, batched=True)\n",
        "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "print(\"âœ… ë°ì´í„° í† í¬ë‚˜ì´ì§• ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "91pbsbwBb4BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 6ï¸âƒ£ í•™ìŠµ ì„¤ì •\n",
        "# ====================================================\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora-korean-chat\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=10,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset\n",
        ")"
      ],
      "metadata": {
        "id": "03XIcgqFb6bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 7ï¸âƒ£ í•™ìŠµ ì‹œì‘\n",
        "# ====================================================\n",
        "print(\"ğŸš€ í•™ìŠµ ì‹œì‘!\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "NGVgw64rb8AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 9ï¸âƒ£ ê°€ì¤‘ì¹˜ ì €ì¥ (Driveì—)\n",
        "# ====================================================\n",
        "model.save_pretrained(SAVE_DIR)\n",
        "tokenizer.save_pretrained(SAVE_DIR)\n",
        "\n",
        "print(f\"âœ… í•™ìŠµ ì™„ë£Œ ë° LoRA ê°€ì¤‘ì¹˜ ì €ì¥ë¨ â†’ {SAVE_DIR}\")"
      ],
      "metadata": {
        "id": "AuEDhV_eb9vG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}